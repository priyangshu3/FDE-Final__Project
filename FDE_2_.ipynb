{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O_aCUILf3ppt"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Kafka\n",
        "!wget -q https://archive.apache.org/dist/kafka/3.4.0/kafka_2.13-3.4.0.tgz\n",
        "!tar -xzf kafka_2.13-3.4.0.tgz\n",
        "!mv kafka_2.13-3.4.0 kafka"
      ],
      "metadata": {
        "id": "3Gabx4wr4Rfr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.4.1-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "id": "uZgPWL714kNb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Python packages\n",
        "!pip install -q kafka-python pyspark findspark confluent-kafka\n",
        "\n",
        "import findspark\n",
        "findspark.init('/content/spark-3.4.1-bin-hadoop3')\n",
        "\n",
        "print(\" Kafka and Spark installed successfully!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWicfO7f6pJO",
        "outputId": "61df338b-5c29-4e73-89fc-fc9c632ea589"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Kafka and Spark installed successfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries\n",
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient\n",
        "from kafka.admin import NewTopic\n",
        "from kafka.errors import TopicAlreadyExistsError\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\" All libraries imported!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2PzV10_6wFN",
        "outputId": "b919ceb1-590c-420c-9993-92c40dd0b926"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All libraries imported!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Phase 2: Starting Kafka Infrastructure...\\n\")\n",
        "\n",
        "# Start Zookeeper\n",
        "zk_process = subprocess.Popen(\n",
        "    ['./kafka/bin/zookeeper-server-start.sh', './kafka/config/zookeeper.properties'],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "print(\" Zookeeper started\")\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDgYkXzZ64dm",
        "outputId": "e27946b6-148a-4d05-97ef-7786160e3332"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Phase 2: Starting Kafka Infrastructure...\n",
            "\n",
            " Zookeeper started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Kafka Server\n",
        "kafka_process = subprocess.Popen(\n",
        "    ['./kafka/bin/kafka-server-start.sh', './kafka/config/server.properties'],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "print(\" Kafka Server started\")\n",
        "time.sleep(10)\n",
        "\n",
        "print(\"\\n Kafka infrastructure ready!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXwWbXIm7CC6",
        "outputId": "6b62a7ad-0f15-445a-8209-ffcfd32c7b72"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Kafka Server started\n",
            "\n",
            " Kafka infrastructure ready!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Phase 3: Creating Kafka Topics...\\n\")\n",
        "\n",
        "class KafkaTopicManager:\n",
        "    \"\"\"Manage Kafka topics for traffic data pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, bootstrap_servers='localhost:9092'):\n",
        "        self.admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
        "\n",
        "    def create_topic(self, topic_name, num_partitions=3, replication_factor=1):\n",
        "        \"\"\"Create Kafka topic with partitioning\"\"\"\n",
        "        topic = NewTopic(\n",
        "            name=topic_name,\n",
        "            num_partitions=num_partitions,\n",
        "            replication_factor=replication_factor\n",
        "        )\n",
        "        try:\n",
        "            self.admin_client.create_topics([topic])\n",
        "            print(f\" Created topic: {topic_name} ({num_partitions} partitions)\")\n",
        "        except TopicAlreadyExistsError:\n",
        "            print(f\"â„¹  Topic {topic_name} already exists\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFKesb4E7Ojj",
        "outputId": "27bbf521-5ebe-4508-9207-88cd088b8990"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Phase 3: Creating Kafka Topics...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "topic_manager = KafkaTopicManager()\n",
        "topic_manager.create_topic('traffic-raw-stream', num_partitions=5)\n",
        "topic_manager.create_topic('weather-raw-stream', num_partitions=3)\n",
        "topic_manager.create_topic('road-sensor-stream', num_partitions=4)\n",
        "topic_manager.create_topic('traffic-enriched-stream', num_partitions=5)\n",
        "topic_manager.create_topic('accident-predictions', num_partitions=3)\n",
        "topic_manager.create_topic('high-risk-alerts', num_partitions=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObyT8mL97YHh",
        "outputId": "c07f6b4e-1b22-4b4f-8eff-6a2fcd69b022"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â„¹  Topic traffic-raw-stream already exists\n",
            "â„¹  Topic weather-raw-stream already exists\n",
            "â„¹  Topic road-sensor-stream already exists\n",
            "â„¹  Topic traffic-enriched-stream already exists\n",
            "â„¹  Topic accident-predictions already exists\n",
            "â„¹  Topic high-risk-alerts already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 4: DATA PRODUCERS (SIMULATING REAL-TIME DATA SOURCES)\n",
        "# ================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 4: REAL-TIME DATA PRODUCERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ----------------- TRAFFIC DATA PRODUCER -----------------\n",
        "class TrafficDataProducer:\n",
        "    \"\"\"\n",
        "    Producer 1: Real-time traffic data from road sensors\n",
        "    Simulates: Traffic volume, speed, road conditions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers='localhost:9092',\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            compression_type='gzip',\n",
        "            batch_size=16384,\n",
        "            linger_ms=10\n",
        "        )\n",
        "        self.locations = [f'LOC_{i:03d}' for i in range(1, 51)]\n",
        "        self.road_types = ['Highway', 'Urban', 'Rural', 'Residential']\n",
        "        self.road_conditions = ['Dry', 'Wet', 'Icy', 'Under Construction']\n",
        "        self.event_count = 0\n",
        "\n",
        "    def generate_traffic_data(self):\n",
        "        \"\"\"Generate realistic traffic data\"\"\"\n",
        "        hour = datetime.now().hour\n",
        "\n",
        "        # Traffic volume varies by time\n",
        "        if 7 <= hour <= 9 or 17 <= hour <= 19:  # Rush hours\n",
        "            traffic_volume = random.randint(70, 100)\n",
        "            avg_speed = random.randint(20, 50)\n",
        "        elif 22 <= hour or hour <= 5:  # Night\n",
        "            traffic_volume = random.randint(0, 30)\n",
        "            avg_speed = random.randint(60, 80)\n",
        "        else:  # Normal hours\n",
        "            traffic_volume = random.randint(30, 70)\n",
        "            avg_speed = random.randint(40, 70)\n",
        "\n",
        "        data = {\n",
        "            'event_id': f'TRF_{self.event_count:08d}',\n",
        "            'location_id': random.choice(self.locations),\n",
        "            'latitude': round(random.uniform(12.9, 13.1), 6),\n",
        "            'longitude': round(random.uniform(77.5, 77.7), 6),\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'road_type': random.choice(self.road_types),\n",
        "            'traffic_volume': traffic_volume,\n",
        "            'avg_speed': avg_speed,\n",
        "            'speed_limit': random.choice([30, 40, 50, 60, 70, 80]),\n",
        "            'road_condition': random.choice(self.road_conditions),\n",
        "            'num_vehicles': random.randint(0, 50),\n",
        "            'heavy_vehicles': random.randint(0, 10),\n",
        "            'construction_zone': random.choice([0, 1]) if random.random() < 0.1 else 0,\n",
        "            'school_zone': random.choice([0, 1]) if random.random() < 0.05 else 0\n",
        "        }\n",
        "\n",
        "        self.event_count += 1\n",
        "        return data\n",
        "\n",
        "    def produce_stream(self, num_events=1000, delay=0.05):\n",
        "        \"\"\"Produce continuous stream of traffic data\"\"\"\n",
        "        print(\"\\n Traffic Producer: Streaming data to Kafka...\\n\")\n",
        "\n",
        "        for i in range(num_events):\n",
        "            data = self.generate_traffic_data()\n",
        "            self.producer.send('traffic-raw-stream', value=data)\n",
        "\n",
        "            if (i + 1) % 200 == 0:\n",
        "                print(f\"   Sent {i + 1} traffic events...\")\n",
        "\n",
        "            time.sleep(delay)\n",
        "\n",
        "        self.producer.flush()\n",
        "        print(f\"\\n Traffic Producer: Sent {num_events} events\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqkjLUN77no6",
        "outputId": "65250eb9-d474-4669-f230-ae5017b7149a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PHASE 4: REAL-TIME DATA PRODUCERS\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- WEATHER DATA PRODUCER -----------------\n",
        "class WeatherDataProducer:\n",
        "    \"\"\"\n",
        "    Producer 2: Real-time weather data\n",
        "    Simulates: Weather conditions, temperature, visibility\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers='localhost:9092',\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            compression_type='gzip'\n",
        "        )\n",
        "        self.locations = [f'LOC_{i:03d}' for i in range(1, 51)]\n",
        "        self.weather_conditions = ['Clear', 'Rain', 'Fog', 'Snow', 'Cloudy']\n",
        "        self.event_count = 0\n",
        "\n",
        "    def generate_weather_data(self):\n",
        "        \"\"\"Generate realistic weather data\"\"\"\n",
        "        weather = random.choice(self.weather_conditions)\n",
        "\n",
        "        # Weather-dependent values\n",
        "        if weather == 'Clear':\n",
        "            visibility = random.randint(80, 100)\n",
        "            temperature = random.uniform(20, 35)\n",
        "            precipitation = 0\n",
        "        elif weather == 'Rain':\n",
        "            visibility = random.randint(30, 70)\n",
        "            temperature = random.uniform(15, 25)\n",
        "            precipitation = random.uniform(5, 30)\n",
        "        elif weather == 'Fog':\n",
        "            visibility = random.randint(10, 40)\n",
        "            temperature = random.uniform(10, 20)\n",
        "            precipitation = 0\n",
        "        elif weather == 'Snow':\n",
        "            visibility = random.randint(20, 50)\n",
        "            temperature = random.uniform(-5, 10)\n",
        "            precipitation = random.uniform(10, 40)\n",
        "        else:  # Cloudy\n",
        "            visibility = random.randint(60, 90)\n",
        "            temperature = random.uniform(18, 28)\n",
        "            precipitation = 0\n",
        "\n",
        "        data = {\n",
        "            'event_id': f'WTH_{self.event_count:08d}',\n",
        "            'location_id': random.choice(self.locations),\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'weather_condition': weather,\n",
        "            'temperature': round(temperature, 2),\n",
        "            'visibility': visibility,\n",
        "            'precipitation': round(precipitation, 2),\n",
        "            'humidity': random.randint(30, 95),\n",
        "            'wind_speed': round(random.uniform(0, 30), 2),\n",
        "            'atmospheric_pressure': round(random.uniform(980, 1020), 2)\n",
        "        }\n",
        "\n",
        "        self.event_count += 1\n",
        "        return data\n",
        "\n",
        "    def produce_stream(self, num_events=500, delay=0.1):\n",
        "        \"\"\"Produce continuous stream of weather data\"\"\"\n",
        "        print(\"\\nðŸ“¤ Weather Producer: Streaming data to Kafka...\\n\")\n",
        "\n",
        "        for i in range(num_events):\n",
        "            data = self.generate_weather_data()\n",
        "            self.producer.send('weather-raw-stream', value=data)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"   Sent {i + 1} weather events...\")\n",
        "\n",
        "            time.sleep(delay)\n",
        "\n",
        "        self.producer.flush()\n",
        "        print(f\"\\n Weather Producer: Sent {num_events} events\\n\")\n"
      ],
      "metadata": {
        "id": "dciPVpEe7xhO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RoadSensorProducer:\n",
        "    \"\"\"\n",
        "    Producer 3: IoT road sensors\n",
        "    Simulates: Road quality, surface conditions, lighting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers='localhost:9092',\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "        )\n",
        "        self.locations = [f'LOC_{i:03d}' for i in range(1, 51)]\n",
        "        self.lighting_conditions = ['Daylight', 'Dark', 'Dusk/Dawn', 'Street Lights']\n",
        "        self.event_count = 0\n",
        "\n",
        "    def generate_sensor_data(self):\n",
        "        \"\"\"Generate road sensor data\"\"\"\n",
        "        hour = datetime.now().hour\n",
        "\n",
        "        # Lighting based on time\n",
        "        if 6 <= hour <= 18:\n",
        "            lighting = 'Daylight'\n",
        "        elif 19 <= hour <= 21 or 5 <= hour <= 6:\n",
        "            lighting = 'Dusk/Dawn'\n",
        "        else:\n",
        "            lighting = random.choice(['Dark', 'Street Lights'])\n",
        "\n",
        "        data = {\n",
        "            'event_id': f'SEN_{self.event_count:08d}',\n",
        "            'sensor_id': f'SENSOR_{random.randint(1, 100):03d}',\n",
        "            'location_id': random.choice(self.locations),\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'lighting_condition': lighting,\n",
        "            'road_surface_quality': random.uniform(0, 10),  # 0=poor, 10=excellent\n",
        "            'lane_markings_visible': random.choice([0, 1]),\n",
        "            'traffic_signal_present': random.choice([0, 1]),\n",
        "            'pedestrian_crossing': random.choice([0, 1]),\n",
        "            'road_width_meters': round(random.uniform(6, 20), 2),\n",
        "            'number_of_lanes': random.randint(1, 4),\n",
        "            'curve_present': random.choice([0, 1]) if random.random() < 0.3 else 0,\n",
        "            'slope_grade': round(random.uniform(-10, 10), 2)\n",
        "        }\n",
        "\n",
        "        self.event_count += 1\n",
        "        return data\n",
        "\n",
        "    def produce_stream(self, num_events=800, delay=0.03):\n",
        "        \"\"\"Produce continuous stream of sensor data\"\"\"\n",
        "        print(\"\\nðŸ“¤ Sensor Producer: Streaming data to Kafka...\\n\")\n",
        "\n",
        "        for i in range(num_events):\n",
        "            data = self.generate_sensor_data()\n",
        "            self.producer.send('road-sensor-stream', value=data)\n",
        "\n",
        "            if (i + 1) % 200 == 0:\n",
        "                print(f\"   Sent {i + 1} sensor events...\")\n",
        "\n",
        "            time.sleep(delay)\n",
        "\n",
        "        self.producer.flush()\n",
        "        print(f\"\\n Sensor Producer: Sent {num_events} events\\n\")\n"
      ],
      "metadata": {
        "id": "nDBkmcHc726N"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start all producers in parallel threads\n",
        "traffic_producer = TrafficDataProducer()\n",
        "weather_producer = WeatherDataProducer()\n",
        "sensor_producer = RoadSensorProducer()\n",
        "\n",
        "traffic_thread = threading.Thread(target=traffic_producer.produce_stream, args=(1000, 0.03))\n",
        "weather_thread = threading.Thread(target=weather_producer.produce_stream, args=(500, 0.06))\n",
        "sensor_thread = threading.Thread(target=sensor_producer.produce_stream, args=(800, 0.04))\n",
        "\n",
        "traffic_thread.daemon = True\n",
        "weather_thread.daemon = True\n",
        "sensor_thread.daemon = True\n",
        "\n",
        "print(\"\\n Starting all producers in parallel...\\n\")\n",
        "traffic_thread.start()\n",
        "weather_thread.start()\n",
        "sensor_thread.start()\n",
        "\n",
        "\n",
        "time.sleep(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z47u9Nvk8Gix",
        "outputId": "1c4ccd7a-b4d3-4b8b-b1b1-1339948b8464"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-23 (produce_stream):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-1326548535.py\", line 68, in produce_stream\n",
            "  File \"/tmp/ipython-input-1326548535.py\", line 46, in generate_traffic_data\n",
            "Exception in thread Thread-24 (produce_stream):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-926756882.py\", line 65, in produce_stream\n",
            "  File \"/tmp/ipython-input-926756882.py\", line 49, in generate_weather_data\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 160, in wrapped\n",
            "Exception in thread Thread-25 (produce_stream):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-4213189503.py\", line 52, in produce_stream\n",
            "  File \"/tmp/ipython-input-4213189503.py\", line 38, in generate_sensor_data\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 160, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/functions.py\", line 3444, in round\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/functions.py\", line 3444, in round\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/utils.py\", line 160, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/functions.py\", line 3444, in round\n",
            "    return _invoke_function(\"round\", _to_java_column(col), scale)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/column.py\", line 65, in _to_java_column\n",
            "    return _invoke_function(\"round\", _to_java_column(col), scale)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/column.py\", line 65, in _to_java_column\n",
            "    raise TypeError(\n",
            "TypeError: Invalid argument, not a string or column: 22.12598967272393 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
            "    return _invoke_function(\"round\", _to_java_column(col), scale)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/column.py\", line 65, in _to_java_column\n",
            "    raise TypeError(\n",
            "TypeError: Invalid argument, not a string or column: 18.20181460679006 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
            "    raise TypeError(\n",
            "TypeError: Invalid argument, not a string or column: 13.017287349612658 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Starting all producers in parallel...\n",
            "\n",
            "\n",
            " Traffic Producer: Streaming data to Kafka...\n",
            "\n",
            "\n",
            "ðŸ“¤ Weather Producer: Streaming data to Kafka...\n",
            "\n",
            "\n",
            "ðŸ“¤ Sensor Producer: Streaming data to Kafka...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 5: SPARK SESSION INITIALIZATION\n",
        "# ================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PHASE 5: INITIALIZING SPARK SESSION\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TrafficAccidentPredictionPipeline\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .config(\"spark.default.parallelism\", \"10\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "print(\"âœ… Spark Session initialized\")\n",
        "print(f\"   Spark Version: {spark.version}\")\n",
        "print(f\"   Available Cores: {spark.sparkContext.defaultParallelism}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnaYpw4f8ZpI",
        "outputId": "3d1d23d2-7ac0-41ae-a90f-9c82eb457560"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PHASE 5: INITIALIZING SPARK SESSION\n",
            "================================================================================\n",
            "\n",
            "âœ… Spark Session initialized\n",
            "   Spark Version: 3.4.1\n",
            "   Available Cores: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # PHASE 6: SPARK STREAMING CONSUMERS - BRONZE LAYER\n",
        "# ================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 6: SPARK STREAMING - BRONZE LAYER (RAW DATA)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"  Reading streams from Kafka...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5WGHn7Y8jFZ",
        "outputId": "0a06d2a6-ac3d-4198-b95e-2d3fd15cddd8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PHASE 6: SPARK STREAMING - BRONZE LAYER (RAW DATA)\n",
            "================================================================================\n",
            "\n",
            "  Reading streams from Kafka...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "xlD7ap2SXXKH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a live stream (acts like Kafka messages)\n",
        "import socket, time, json, threading, random\n",
        "\n",
        "def accident_stream_server():\n",
        "    server = socket.socket()\n",
        "    server.bind((\"localhost\", 9999))\n",
        "    server.listen(1)\n",
        "    conn, _ = server.accept()\n",
        "    while True:\n",
        "        msg = {\n",
        "            \"timestamp\": time.time(),\n",
        "            \"location\": random.choice([\"Delhi\",\"Mumbai\",\"Chennai\",\"Kolkata\"]),\n",
        "            \"weather\": random.choice([\"Rainy\",\"Clear\",\"Foggy\",\"Sunny\"]),\n",
        "            \"vehicles_involved\": random.randint(1,5),\n",
        "            \"severity\": random.choice([\"Low\",\"Medium\",\"High\"])\n",
        "        }\n",
        "        conn.send((json.dumps(msg) + \"\\n\").encode(\"utf-8\"))\n",
        "        time.sleep(2)\n",
        "\n",
        "threading.Thread(target=accident_stream_server).start()\n"
      ],
      "metadata": {
        "id": "cw8gnS5PXdHb"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AccidentPipeline\").getOrCreate()\n",
        "\n",
        "schema = (StructType()\n",
        "          .add(\"timestamp\", DoubleType())\n",
        "          .add(\"location\", StringType())\n",
        "          .add(\"weather\", StringType())\n",
        "          .add(\"vehicles_involved\", IntegerType())\n",
        "          .add(\"severity\", StringType()))\n",
        "\n",
        "bronze = (spark.readStream\n",
        "          .format(\"socket\")\n",
        "          .option(\"host\", \"localhost\")\n",
        "          .option(\"port\", 9999)\n",
        "          .load())\n",
        "\n",
        "from pyspark.sql.functions import from_json, col\n",
        "\n",
        "bronze_parsed = bronze.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
        "\n",
        "\n",
        "bronze_query = (bronze_parsed\n",
        "                .writeStream\n",
        "                .format(\"parquet\")\n",
        "                .option(\"path\", \"/content/bronze\")\n",
        "                .option(\"checkpointLocation\", \"/content/chk_bronze\")\n",
        "                .outputMode(\"append\")\n",
        "                .start())\n"
      ],
      "metadata": {
        "id": "39BAm3RGXlux"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "silver = (bronze_parsed\n",
        "          .withColumn(\"weather\", when(col(\"weather\")==\"Foggy\", \"Low Visibility\").otherwise(col(\"weather\")))\n",
        "          .withColumn(\"accident_index\", col(\"vehicles_involved\") *\n",
        "                      when(col(\"severity\")==\"High\", 3)\n",
        "                      .when(col(\"severity\")==\"Medium\", 2)\n",
        "                      .otherwise(1)))\n",
        "\n",
        "silver_query = (silver\n",
        "                .writeStream\n",
        "                .format(\"parquet\")\n",
        "                .option(\"path\", \"/content/silver\")\n",
        "                .option(\"checkpointLocation\", \"/content/chk_silver\")\n",
        "                .outputMode(\"append\")\n",
        "                .start())\n"
      ],
      "metadata": {
        "id": "VFB6kBExXvZL"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, from_unixtime, avg, count, window\n",
        "\n",
        "\n",
        "silver_fixed = silver.withColumn(\"event_time\", from_unixtime(col(\"timestamp\")).cast(\"timestamp\"))\n",
        "\n",
        "\n",
        "gold = (silver_fixed\n",
        "        .withWatermark(\"event_time\", \"1 minute\")\n",
        "        .groupBy(\n",
        "            window(col(\"event_time\"), \"1 minute\"),\n",
        "            col(\"location\")\n",
        "        )\n",
        "        .agg(\n",
        "            avg(\"accident_index\").alias(\"avg_accident_index\"),\n",
        "            count(\"*\").alias(\"record_count\")\n",
        "        )\n",
        "        .select(\n",
        "            col(\"window.start\").alias(\"window_start\"),\n",
        "            col(\"window.end\").alias(\"window_end\"),\n",
        "            col(\"location\"),\n",
        "            col(\"avg_accident_index\"),\n",
        "            col(\"record_count\")\n",
        "        ))\n",
        "\n",
        "\n",
        "gold_query = (gold\n",
        "              .writeStream\n",
        "              .format(\"parquet\")\n",
        "              .option(\"path\", \"/content/gold\")\n",
        "              .option(\"checkpointLocation\", \"/content/chk_gold\")\n",
        "              .outputMode(\"append\")\n",
        "              .start())\n"
      ],
      "metadata": {
        "id": "4n5bEggqYyA7"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}